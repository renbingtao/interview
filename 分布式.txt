
========================认识分布式系统========================

分布式系统通常具有的特点
多进程
不共享操作系统，通过网络通信传递消息来协作
不共享时钟，很难只通过时间来定义两个事件的顺序

分布式计算的问题
网络不可靠
有延迟
部分失效
时钟问题

网络延迟
我们认为请求丢失了，但消息只是延迟到达
消息可能以不同的顺序到达，或者不同节点上消息到达的顺序不同

部分失效问题
分布式系统中，可能一部分节点正常工作，另一部分停止运行（或者正常运行但由于网络中断无法协同工作）

时钟问题
每台机器都有自己的时钟，各个物理设备的本地时钟走时并不准确，可能比其他机器快或慢

========================分布式系统模型========================

两将军问题
两个将军想要进攻一座城市，必须同时进攻，若仅一方进攻则失败。双方通过信使交流信息，但信使可能被俘获
两将军问题已被证明无解，但计算机科学家们找到了工程上的解决方案，比如TCP三次握手

拜占庭将军
两将军问题的拓展，可以有三个将军。该问题的挑战在于，将军中可能出现叛徒。比如C从A和B收到了互相矛盾的消息，C无法确定谁是叛徒。第一种可能是A告诉BC都进攻，但B告诉C撤退，即B撒谎。另一种可能是A发送了互相矛盾的命令，告诉C进攻却告诉B撤退，即A撒谎

网络链路模型
在分布式系统中，网络出错导致的问题称为网络分区，即由于网络设备故障，导致网络分裂为多个独立的组。也就是说节点仍正常工作，但它们之间的通信连接已经中断。通常讨论网络分区时，默认网络中断会存在一段时间
大多数分布式算法都假设网络是点对点的单播通信。我们假设有一个发送者和接收者，它们通过一个双向的链路通信。一个链路有两个基本事件：发送事件，即将一条消息发送到链路上；接收事件，链路返回一条消息

我们把网络链路分为以下几种：
可靠链路：可靠传递（每个消息都会被链路传递）；没有重复；不会无中生有
公平损失链路：公平损失（若收发方都是正常运行，且发送方不断重复发送消息，则消息终会送达）；有限重复；不会无中生有
任意链路：允许任意的网络链路执行任何操作，可能修改数据包

按时间划分系统模型
基于时间是否同步，分为同步系统模型和异步系统模型
这里的同步指，一个消息的响应时间在一个有限且已知的时间范围内。而异步指，一个消息的响应时间是无限的，无法知道一条消息何时到达

========================分布式数据基础========================

分区分为垂直分区和水平分区（或分片）

水平分区算法

范围分区：根据指定的关键字将数据集拆分为若干连续的范围，每个范围存储到一个单独的节点上
优点：实现简单；能够根据分区键进行范围查询；通过修改范围边界增加或减少范围数据，实现简单有效地调整范围（重新分区）
缺点：无法通过分区键之外的关键字进行范围查询；查询范围较大且位于多个节点时，性能差；数据分布不均时，某些节点负载很高，即某些数据的热点现象

哈希分区：对关键字进行hash函数计算，根据计算的值决定数据集的分区
优点：数据几乎是随机分布的，能避免热点问题
缺点：无法进行范围查询；添加或删除节点时，引起数据大规模移动

一致性哈希：特殊的hash分区算法，用来缓解增加或删除节点时引起的大规模数据移动问题
1.将整个哈希值组织成一个抽象的圆环，称为哈希环，范围一般在0-int最大值，这些输出值均匀地映射到哈希环边上
2.将分布式系统的节点映射到圆环上。比如有三个节点，则两两节点之间间隔120度
3.根据要存储的数据的关键字，计算哈希值，映射到hash环上，按【顺时针】方向，将数据保存在遇到的【第一个节点】上
当添加节点时，从新节点开始逆时针到上一个节点，将包含的数据移动到新节点。删除节点同理
问题是删除节点时，该节点的数据会全部倾斜到顺时针的下一个节点
解决方法是引入虚拟节点：引入多个虚拟节点，一个物理节点对应多个虚拟节点，数据根据虚拟节点分布。虚拟节点越多，数据分布越均匀。此外，若节点的配置、性能不同，通过分配映射的虚拟节点的数量，让高性能的节点承受更高的负载

复制有三种常用的类型：单主复制，多主复制，无主复制

单主复制即一个副本为主节点，其他为从节点
多主复制即多个节点充当主节点。常用于多个数据中心的存储系统，避免写请求跨数据中心。比如有一个全球服务，在全球有多个数据中心，根据请求路由到地理位置更近的数据中心，加快访问速度
在基于领导者的复制体系中，必须由领导者来确认写请求，若领导者没有响应，则整个系统无法正常工作

无主复制
没有主节点，基本思想是，客户端不仅向一个节点发送写请求，而是将请求发送到多个甚至全部节点，一旦得到部分节点的确认，就认为写成功。读请求也会发送给多个节点，根据节点的数据及数据版本号，决定使用哪个值
无主复制有多种协调方式：一种是客户端直接把写操作发送给多个副本；另一种是在节点中选举一个协调节点，客户端把请求发送到协调节点，由该节点代表客户端把操作转发到多个副本，多副本确认后由协调节点响应客户端
无主复制的优势是可以轻松容忍节点故障：由于去掉了领导者，只要有足够多的节点可以写入，系统仍然是正常的
修复旧数据（保持一致）的两种方式
1.让客户端负责更新数据，即从多个节点读取数据后，若检测到某节点的数据为旧数据，则更新该节点的数据为新数据
2.反熵过程。新建一个后台进程来修复数据，该进程找出错误数据，从保存最新数据的节点中将数据复制到错误节点。该过程不保证写操作的顺序，只保证最后结果一致
使用了Merkle树（也叫哈希树）来验证数据是否一致。它把数据按关键字分为几个范围，每个范围计算一个hash值作为叶子节点；叶子节点的值拼接后再计算hash值作为父节点，以此类推合并到根节点。该树的特点是每个分支可以独立比对。若根节点相同，则两颗树相同；若不同则传输子节点进行比对，一直到关键字所在范围
法人机制是用来保证数据冗余和最终一致性的算法。前面提到的客户端要向一些节点发送读写请求，法人机制就用来确定需要多少节点
系统由N个节点组成，我们要求至少W个节点写入成功，读取从R个节点读取。则W+R>N且W>N/2

CAP
正常情况下，一个集群内的节点之间是可以正常通信的，异常时某些节点之间无法通信，可能导致无法获取需要的数据，这是无法容忍的，因此把节点多复制几份，一个节点无法访问，但可以访问其他节点的，这就是分区容错性
然而要保证多个节点之间数据是相同的，这就带来了一致性问题，要保证一致性，写数据时要保证所有节点都写成功
保证一致性又会带来可用性问题，因为操作时间变长了。
总之，节点越多，分区容错性越高；分区容错高了，一致性又难保证；一致性保证了，可用性又难保证

PACELC
PACELC定理是CAP定理的一个拓展定理，其主要论点是，CAP忽略分布式系统中的延迟影响是一个重大疏忽，因为系统运行时延迟时刻存在，而网络分区不会一直存在。该定理指出，在分布式系统存在网络分区的情况下，必须在可用性（A）和一致性（C）之间做出选择，否则（没有网络分区且正常运行）在延迟（L）和一致性（C）之间做出选择
                分区
        是                   否
可用性     一致性     延迟      一致性
PA          PC        EL        EC
这四种子类别可以组合起来。比如PA/EL类型的系统在网络分区时优先考虑可用性，而系统正常运行时优先考虑延迟。大多数系统属于PA/EL或PC/EC类型

BASE
对CAP理论的一致性和可用性权衡的结果，系统无法做到强一致性，允许暂时不一致，但可以达到最终一致性
基本原则：
基本可用（Basically Available）:系统在大部分时间内是可用
软状态（Soft state）：节点的数据可以处于不一致的状态，但不会影响系统的整体可用性
最终一致性（Eventual consistency）：保证所有节点的数据最终会达到一致状态

线性一致性
是最强的一致性模型，也称为强一致性，CAP的一致性就是线性一致性。这意味着分布式系统的所有操作看起来都是原子的，整个分布式系统看起来好像只有一个节点
严格定义是：给定一个执行历史，执行历史根据并发操作可以拓展为多个顺序历史，只要从中找到一个合法的顺序历史，那么该执行历史就是线性一致性的
执行历史转变为顺序历史有三种情况：
一个操作明显在另一个操作之前发送，则它们是顺序关系
两个操作之间有重叠，则它们是并发关系
一个操作包含另一个操作，则它们是并发关系
在转变过程中，若两个操作是顺序关系，则它们的顺序关系要保持相同；若是并发关系，则他们可以按任何顺序排列

举例:初始x=0
A           write(1)
B               write(3)    read(1)
上述执行历史可以拓展为以下两个顺序历史
S1  write(1)    write(3)    read(1)
S2  write(3)    write(1)    read(1)
S1不合法，应为write(3)后read应返回3。S2合法

========================分布式共识========================

分布式系统中的几个主要难题，比如网络分区、时钟不一致等，可以通过状态机复制来解决
多个相同状态机的副本，从同样的初始状态开始，经历相同的输入序列后，达到相同的状态，输出相同的结果
实现状态机复制常常需要一个多副本日志系统，这是从日志系统收到了启发：如果日志的内容和顺序都相同，多个进程从同一状态开始，从相同的位置以相同的顺序读取日志内容，则这些进程将生成相同的输入，并以同样的状态结束
共识算法常用来实现多副本日志，共识算法使得每个副本对日志的值和顺序达成共识，每个节点都存储相同的日志副本，最终这些节点看起来就像一个单独的、高可用的状态机

========================raft算法========================

raft是一种基于领导者的共识算法，用来保证日志完全相同地复制到多台服务器上，以实现状态机复制
系统模型为：服务器可能宕机，但不存在拜占庭故障，即节点行为非恶意，不会篡改数据；消息可能丢失、延迟、乱序、重复，有可能存在网络分区，并在一段时间后恢复
raft比较简单，多个产品都实现，如nacos，etcd

raft通过选举出一个leader，让他全权负责管理日志复制来实现一致性。leader从客户端接收日志条目，把日志条目复制到其他follower上，并在安全的时候通知他们应用到状态机中
raft将一致性问题分解为三个子问题：leader选举，日志复制，安全性

raft基础
一个Raft集群包含若干个服务器节点；通常是5个，这样的系统可以容忍2个节点的失效
raft算法中的服务器只能处于以下三种状态
leader：处理所有客户端请求（如果一个客户端和 follower 通信，follower 会将请求重定向给 leader）和日志复制，同一时刻最多只能有一个正常工作的领导者
follower：完全被动地处理请求，即不主动发送RPC请求，只对RCP请求进行响应，服务器大多数情况下处于此状态
candidate：用来选举出新的领导者，处于领导者和跟随者之间的暂时状态

raft选出领导者意味着进入一个新的任期，即逻辑时间，用数字表示，初始值为0，递增。raft将分布式系统中的时间划分成不同的任期来解决时序问题
一个正常的任期分为两部分：选举过程和正常运行过程。某些任期内没有选举出领导者，则会立即进入下一个任期，尝试选出一个领导者
每台服务器需要维护一个currentTerm变量，表示服务器当前已知的最新任期号，该变量需持久化，以便重启时能够知道最新任期
服务器之间通信时会交换任期，若某个服务器的任期小于其他服务器，则会更新为较大的值；如果leader或candidate发现有其他任期更大的服务器，会退回到follower。若服务器收到的请求的任期小于当前服务器，则会拒绝该请求

服务器之间的通信主要通过两个RCP实现：RequestVote RPC（用于领导者选举）和AppendEntries RPC（领导者用来复制日志和发送心跳）

leader选举
每个节点刚启动时都是follower，follower需要在选举超时时间（electionTimeout，100-500ms）接收leader心跳信息（空的AppendEntries消息）或者候选人投票请求，若能收到，则继续当follower；若收不到，则认为当前集群里没有leader，发起选举

节点开始竞选时，转变成候选人（candidate），并把自己的任期+1，并给自己投一票，并给其他节点发出投票请求（RequestVote），然后在一段时间内等待投票结果。投票结果有三种
1.候选人得到超过半数票数，成为leader，通知其他节点（AppendEntries）
2.收到来自领导者的AppendEntries消息，即同任期内已经有其他leader或者有更高任期的leader，退回为follower
3.选举等待超时，候选人即无法成为leader，也没有接收到其他leader的通知，会开启下一轮选举：任期+1，票数重置为1，向其他节点发送投票请求。每个节点内部的定时器的到期时间不一致，避免多个节点同时发起投票，导致票数分散，选举失败

选举过程中需要保持两个特性：安全性和活性
安全性指一个任期内只会有一个领导者被选举出来，需要保证：1.每个节点在同一任期内只能投一次票，它将投给第一个满足条件的RequestVote请求，然后拒绝其他候选者的请求 2.只有获得超过半数（刚好半数不行）的选票才能成为leader
活性指系统最终能选出一个leader，节点的选举超时时间可以使用随机时间，通常在[T,2T]之内，由于随机性节点不太可能同时开始竞选，因此有足够的时间获取其他节点的选票

日志复制
leader被选举出来后，就可以对客户端提供服务，客户端的每个请求都包含一条被复制状态机执行的指令。leader把该指令作为一个新的条目追加到日志中，然后并行发起AppendEntries RPC给其他服务器，让它们复制该条目。当条目被安全地复制，leader会应用该条目到状态机中（执行指令）并把执行结果返回给客户端。如果follower崩溃或执行缓慢，或网络丢包，leader会不断重试RPC直到所有的follower都保存了条目
每个节点存储自己的日志副本，日志中的每个日志条目包含以下：索引、任期号、命令
raft算法通过索引+任期号唯一标识一条日志记录
日志必须持久化存储，一个节点必须先将日志条目安全地写入磁盘中，才能向系统中其他节点发送请求或回复请求
一旦一条日志条目被存储在超过半数的节点上，则认为该记录【已提交】，同时leader中该日志之前的日志也会被提交，包括由其他leader创建的条目。已提交意味着可以安全地执行记录
如果两个节点的日志在相同索引上的任期号相同，则认为它们具有一样的命令，且从日志开头到这个索引之间的日志也完全相同
AppendEntries消息包含新条目的上一个条目的索引及任期，leader把自己的最后一个条目的索引及任期和请求相比较，若相同则接收记录，否则拒绝（日志一致性检查）

新的leader上任后，日志可能不一致，比如老的leader还没有完全复制日志里的所有条目就崩溃了，follower可能缺少新leader的条目，也可能包含新leader没有的条目，也可能同时发生
在raft算法中，leader通过强制follower复制它的日志来解决不一致的问题，即follower中和leader不一致的条目会被覆盖
leader会找到两者达成一致的最大的索引，删除follower该索引后的所有日志，并把自己的那个索引后的日志条目发送给follower，这些操作都发生在AppendEntries RPC中，即新leader上任后不会专门手动清理日志，而是在正常的RPC调用时清理
leader对每个follower都维护了一个【nextIndex】，表示leader要发送给follower的下一个日志条目的索引，当新leader产生时，该leader的所有nextIndex都会初始化为它自己最后一个日志条目的index+1
若leader和follower的日志不一致，则在RPC的一致性检查时会失败，则leader把nextIndex-1并重试RPC，最终nextIndex会在某个位置达成一致，此时RPC成功，follower中冲突的日志全部删除，追加leader的日志

安全性
上面的做法有个前提，即leader的日志需要始终是对的，raft通过安全性来保证这一点，内容如下
选举限制
候选人为了赢得选举必须与集群中的过半节点通信，这意味着至少其中一个服务器节点包含了所有已提交的日志条目。如果 candidate 的日志至少和过半的服务器节点一样新（接下来会精确地定义“新”），那么他一定包含了所有已经提交的日志条目。RequestVote RPC 执行了这样的限制： RPC 中包含了 candidate 的日志信息，如果投票者自己的日志比 candidate 的还新，它会拒绝掉该投票请求
raft 通过比较两份日志中最后一条日志条目的索引值和任期号来定义谁的日志比较新。如果两份日志最后条目的任期号不同，那么任期号大的日志更新。如果两份日志最后条目的任期号相同，那么日志较长的那个更新

========================架构的演化历史========================

单体
ALL IN ONE，所有功能模块都在一个应用里，只有一个节点，申请公网域名，域名绑定单一节点的ip
优点：开发部署简单
缺点：无法应对高并发

集群
对单个节点复制，多个节点共同工作。由于域名只能指向一个节点，其他副本无法接受请求，因此引入了网关，网关是所有请求的入口，一般为服务器，里面部署了ng。域名绑定了网关的ip，网关把请求转发（路由）到副本，为了防止请求全都转发给同一台服务器，引入了负载均衡。当并发过大，可以通过增加副本的数量的方式（扩容）来提高应用的性能
优点：集群可以解决大并发的问题
缺点：模块升级时，整个系统都需要重新部署，牵一发而动全身

分布式
按照模块，将整个应用拆分为多个小应用（微服务），同理数据库也可拆分，每个微服务对应有自己的数据库（分库），服务之间通过接口而非数据库直连，每个微服务可以用不同的语言开发（自治）。服务之间调用时，不能写死ip，因此需要服务注册中心将服务与ip列表绑定。此外注册中心还可以管理配置，传统模式下更新了配置需要重新发布部署，有了配置中心后只需要在配置中心统一修改，配置中心会把配置的变动主动推送到微服务。当某个底层服务异常，所有依赖这个服务的上层服务也会出现响应慢，不可用，因此需要服务熔断机制，将异常的服务快速失败（通过配置，比如5秒内50%的请求都卡顿，则后5秒内的请求都失败）。由于任意节点都无法提供完整的服务，因此需要一个网关，根据请求路径从注册中心找到对应的服务，然后路由。由于数据库分库，因此需要实现分布式事务

========================分布式幂等性========================

查询和删除不在讨论范围之内
1.建立唯一索引
2.token机制。进入下单页前端请求后台，获取token，后台将token放入redis并设置ttl。用户下单时带上token，后台校验token是否有效，有效时更新redis的状态为处理中，然后执行业务代码，这样当用户误点击导致重复提交时，后台校验redis发现状态为处理中，则返回请勿重复提交

========================限流算法========================

计数器算法（固定窗口）：使用计数器在周期内累加，达到限流值时丢弃，下一周期开始时清零
滑动窗口：将周期分为多个小周期，分别记录每个小周期内的次数，并根据时间删除小周期
漏桶：一个底部有洞的桶，请求像水一样注入到桶中，以恒定的速率流出桶，桶满时丢弃，一般用队列实现
令牌桶：以固定的速度向令牌桶中扔令牌，直到桶满；请求需要先获取令牌才能继续，否则被丢弃

========================共识算法及Raft协议========================

定义
分布式系统中节点如何达成一致性的算法。常见算法为Paxos和Raft

Raft比较简单，多个产品都实现，如nacos，tidb，etcd，kafka，rabbitmq
raft用来保证日志完全相同地复制到多台服务器上，以实现状态机复制
raft把共识问题分解为：领导者选举、日志复制、安全性。节点之间使用RPC通信，RPC主要包括两种：RequestVote RPC（投票）和AppendEntries RPC（复制日志和心跳检测）

任期
任意节点在leader故障后都有可能成为候选人，因此每个节点都需要知道现在的任期（任意长度的时间，用递增的数字来表示）。Raft是强领导模型，即领导者在时，才能对外提供服务，因此选举期间不能提供服务

日志
由索引、任期、指令组成。索引是严格地增大的。示例
Index       1       2       3       4       5
Term        1       1       1       2       3
Command     x <- 3  y <- 1  y <- 9  x <- 2  x <- 0
Raft算法通过索引和任期号唯一标识一条日志记录
日志必须持久化，节点必须先将日志条目安全写到磁盘中，才能向系统中其他节点发送请求或回复请求
如果一条日志条目被存储在超过半数的节点上，则认为该记录【已提交】，意味着状态机可以安全地执行该记录，这条记录不能再改变了

候选人
在任意时刻，每个节点都是leader、follower、candidate三种状态之一。每个节点刚启动时都是follower，follower需要每隔一段时间（每个节点内部都有一个定时器）接收leader心跳信息或者候选人投票请求，若能收到，则继续当follower；若收不到，则认为当前集群里没有leader，发起选举，变成候选人（candidate），并把自己的任期+1，并给自己投一票，并给其他节点发出投票请求，然后在一段时间内等待投票结果。投票结果有三种：1.候选人得到超过半数票数，成为leader，通知其他节点 2.同任期内已经有其他leader或者有更高任期的leader，退回为follower 3.选举等待超时，候选人即无法成为leader，也没有接收到其他leader的通知，会开启下一轮选举：任期+1，票数重置为1，向其他节点发送投票请求。每个节点内部的定时器的到期时间不一致，避免多个节点同时发起投票，导致票数分散，选举失败

leader
定期向其他follower发送心跳，防止选举
日志叠加，即leader只能追加日志，不能重写或删除日志

投票
任期高的不投给任期低的；日志索引高的不投给日志索引低的节点。这是为了保证只有日志最完整的节点可以成为leader
满足上述条件后，优先投给最先发起投票请求的候选人
每个节点在一个任期内，只能投一次票

日志复制
Raft是强领导者模型，即只有领导可以接受客户端的写请求。leader收到写请求后，把指令写入log（第一阶段），然后发送日志复制请求（复制日志的RPC）给其他follower。如果leader收到超过半数的成功回复，就会执行这条指令，改变自己的数据状态机（第二阶段），并回复成功给客户端，然后向其他follower发送提交通知。Raft不允许跳过中间日志而直接复制最新的日志，比如leader发送索引为8的日志给follower，但follower的索引为5，因此8会被拒绝，leader需要先从6开始，将丢失的日志重新同步。即raft集群要对外提供服务，需要一半以上的节点拥有完整的日志。leader发送心跳或日志复制的RPC消息时会携带日志信息，如果某个follower执行失败导致日志缺失，会重新同步，达到最终一致性。另外建议读请求也由leader处理，如果由follower处理，可能出现follower还未同步leader日志就处理读请求，而出现不一致的现象

脑裂
节点间网络不通（分区故障）时，集群会分裂成两个小集群，有两个领导。当分区恢复时，任期低的leader会成为任期高leader的follower，保证只有一个leader。由于超过半数的节点执行成功时leader才能返回成功，因此只有一个leader能够继续对外提供服务，其他leader只能回复失败

========================本地消息表========================

将分布式事务拆分成本地事务进行处理，在业务数据库中创建本地消息表，利用本地事务的原子性保证业务表和消息表的一致性，再通过mq异步处理这些消息，实现最终一致性
