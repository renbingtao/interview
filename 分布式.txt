
========================架构的演化历史========================

单体
ALL IN ONE，所有功能模块都在一个应用里，只有一个节点，申请公网域名，域名绑定单一节点的ip
优点：开发部署简单
缺点：无法应对高并发

集群
对单个节点复制，多个节点共同工作。由于域名只能指向一个节点，其他副本无法接受请求，因此引入了网关，网关是所有请求的入口，一般为服务器，里面部署了ng。域名绑定了网关的ip，网关把请求转发（路由）到副本，为了防止请求全都转发给同一台服务器，引入了负载均衡。当并发过大，可以通过增加副本的数量的方式（扩容）来提高应用的性能
优点：集群可以解决大并发的问题
缺点：模块升级时，整个系统都需要重新部署，牵一发而动全身

分布式
按照模块，将整个应用拆分为多个小应用（微服务），同理数据库也可拆分，每个微服务对应有自己的数据库（分库），服务之间通过接口而非数据库直连，每个微服务可以用不同的语言开发（自治）。服务之间调用时，不能写死ip，因此需要服务注册中心将服务与ip列表绑定。此外注册中心还可以管理配置，传统模式下更新了配置需要重新发布部署，有了配置中心后只需要在配置中心统一修改，配置中心会把配置的变动主动推送到微服务。当某个底层服务异常，所有依赖这个服务的上层服务也会出现响应慢，不可用，因此需要服务熔断机制，将异常的服务快速失败（通过配置，比如5秒内50%的请求都卡顿，则后5秒内的请求都失败）。由于任意节点都无法提供完整的服务，因此需要一个网关，根据请求路径从注册中心找到对应的服务，然后路由。由于数据库分库，因此需要实现分布式事务

========================对CAP的理解========================

正常情况下，一个集群内的节点之间是可以正常通信的，异常时某些节点之间无法通信，可能导致无法获取需要的数据，这是无法容忍的，因此把节点多复制几份，一个节点无法访问，但可以访问其他节点的，这就是分区容错性
然而要保证多个节点之间数据是相同的，这就带来了一致性问题，要保证一致性，写数据时要保证所有节点都写成功
保证一致性又会带来可用性问题，因为操作时间变长了。
总之，节点越多，分区容错性越高；分区容错高了，一致性又难保证；一致性保证了，可用性又难保证

分布式幂等性如何设计
查询和删除不在讨论范围之内
1.建立唯一索引
2.token机制。进入下单页前端请求后台，获取token，后台将token放入redis并设置ttl。用户下单时带上token，后台校验token是否有效，有效时更新redis的状态为处理中，然后执行业务代码，这样当用户误点击导致重复提交时，后台校验redis发现状态为处理中，则返回请勿重复提交

限流算法
计数器算法（固定窗口）：使用计数器在周期内累加，达到限流值时丢弃，下一周期开始时清零
滑动窗口：将周期分为多个小周期，分别记录每个小周期内的次数，并根据时间删除小周期
漏桶：一个底部有洞的桶，请求像水一样注入到桶中，以恒定的速率流出桶，桶满时丢弃，一般用队列实现
令牌桶：以固定的速度向令牌桶中扔令牌，直到桶满；请求需要先获取令牌才能继续，否则被丢弃

========================对BASE的理解========================

对CAP理论的一致性和可用性权衡的结果，系统无法做到强一致性，允许暂时不一致，但可以达到最终一致性

基本原则
基本可用（Basically Available）:系统在大部分时间内是可用
软状态（Soft state）：节点的数据可以处于不一致的状态，但不会影响系统的整体可用性
最终一致性（Eventual consistency）：保证所有节点的数据最终会达到一致状态

========================共识算法及Raft协议========================

定义
分布式系统中节点如何达成一致性的算法。常见算法为Paxos和Raft

相关背景
复制状态机：核心理念是相同的初始状态+相同的输入=相同的结果状态。通过共识算法，可以实现复制状态机（多个节点从相同的初始状态开始，顺序执行相同的命令，得到相同的结果）

Raft比较简单，多个产品都实现，如nacos，tidb，etcd，kafka，rabbitmq。raft把共识问题分解为：领导者选举、日志复制、安全性。节点之间使用RPC通信，RPC主要包括两种：投票和复制日志

任期
任意节点在leader故障后都有可能成为候选人，因此每个节点都需要知道现在的任期（任意长度的时间，用递增的数字来表示）。Raft是强领导模型，即领导者在时，才能对外提供服务，因此选举期间不能提供服务

日志
由索引、任期、指令组成。索引是严格地增大的。示例
Index       1       2       3       4       5
Term        1       1       1       2       3
Command     x <- 3  y <- 1  y <- 9  x <- 2  x <- 0

候选人
在任意时刻，每个节点都是leader、follower、candidate三种状态之一。每个节点刚启动时都是follower，follower需要每隔一段时间（每个节点内部都有一个定时器）接收leader心跳信息或者候选人投票请求，若能收到，则继续当follower；若收不到，则认为当前集群里没有leader，发起选举，变成候选人（candidate），并把自己的任期+1，并给自己投一票，并给其他节点发出投票请求，然后在一段时间内等待投票结果。投票结果有三种：1.候选人得到超过半数票数，成为leader，通知其他节点 2.同任期内已经有其他leader或者有更高任期的leader，退回为follower 3.选举等待超时，候选人即无法成为leader，也没有接收到其他leader的通知，会开启下一轮选举：任期+1，票数重置为1，向其他节点发送投票请求。每个节点内部的定时器的到期时间不一致，避免多个节点同时发起投票，导致票数分散，选举失败

leader
定期向其他follower发送心跳，防止选举
日志叠加，即leader只能追加日志，不能重写或删除日志

投票
任期高的不投给任期低的；日志索引高的不投给日志索引低的节点。这是为了保证只有日志最完整的节点可以成为leader
满足上述条件后，优先投给最先发起投票请求的候选人
每个节点在一个任期内，只能投一次票

日志复制
Raft是强领导者模型，即只有领导可以接受客户端的写请求。leader收到写请求后，把指令写入log（第一阶段），然后发送日志复制请求（复制日志的RPC）给其他follower。如果leader收到超过半数的成功回复，就会执行这条指令，改变自己的数据状态机（第二阶段），并回复成功给客户端，然后向其他follower发送提交通知。Raft不允许跳过中间日志而直接复制最新的日志，比如leader发送索引为8的日志给follower，但follower的索引为5，因此8会被拒绝，leader需要先从6开始，将丢失的日志重新同步。即raft集群要对外提供服务，需要一半以上的节点拥有完整的日志。leader发送心跳或日志复制的RPC消息时会携带日志信息，如果某个follower执行失败导致日志缺失，会重新同步，达到最终一致性。另外建议读请求也由leader处理，如果由follower处理，可能出现follower还未同步leader日志就处理读请求，而出现不一致的现象

脑裂
节点间网络不通（分区故障）时，集群会分裂成两个小集群，有两个领导。当分区恢复时，任期低的leader会成为任期高leader的follower，保证只有一个leader。由于超过半数的节点执行成功时leader才能返回成功，因此只有一个leader能够继续对外提供服务，其他leader只能回复失败

========================本地消息表========================

将分布式事务拆分成本地事务进行处理，在业务数据库中创建本地消息表，利用本地事务的原子性保证业务表和消息表的一致性，再通过mq异步处理这些消息，实现最终一致性
