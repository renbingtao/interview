
keyword
keyword 是一种数据类型，用于存储结构化的文本数据（如标签、URL、邮箱、状态码等），其特点是不进行分词，直接将整个文本作为一个完整的词条进行索引。这使得 keyword 类型非常适合用于过滤、排序、聚合等操作。
核心特点
1.不分词（Exact Value）
输入的文本会被完整保留，不会被拆分为多个词条。例如，"New York" 会被索引为一个整体，而不是 "New" 和 "York"。
2.支持精确匹配
适合用于精确查找（如 term 查询）、分组统计（聚合）和排序。
3.默认限制长度
单个 keyword 字段的最大长度默认为 32766 字节（32mb），超过会导致索引失败（可通过 ignore_above 参数调整）。

与 text 类型的关键区别
特性	                    keyword	                        text
分词处理	                    不分词，保留完整文本	            分词（根据分词器拆分为多个词条）
适合操作	                    精确匹配、过滤、排序、聚合	        全文搜索（模糊匹配）
索引大小	                    通常较小（词条少）	            通常较大（词条多）
默认搜索方式	                精确匹配（term 查询）	            模糊匹配（match 查询）

若需同时支持全文搜索和精确匹配，可在 text 字段下添加 keyword 子字段（如 name.keyword）。
id类型的字段几乎全部都使用keyword

scroll
适合海量数据导出，不适合实时查询。拍快照，不能反映最新数据。适用于离线分析，全量数据同步

分页
若需实时分页，且结果集较小（< 10,000），可使用 from/size（但深度分页性能差）。官方建议避免超过 10,000 条记录的深度分页
理论上支持任意的偏移量，但实际系统有限制（默认from超过10000时会报错，可以调整系统参数来解除限制，但会显著增加集群内存和 CPU 消耗，不建议在生产环境中操作。）
按价格降序排列，获取第 51-60 条记录：
GET /products/_search
{
  "from": 50,
  "size": 10,
  "sort": [
    {
      "price": {
        "order": "desc"
      }
    }
  ],
  "query": {
    "term": {
      "category": "electronics"
    }
  }
}

深分页
ES 5.0+ 推荐使用 search_after 参数。search_after 是一种高性能的分页方式，用于替代传统的 from/size 分页。它通过维护上一页的排序值（而非页码）来实现高效的深度分页，特别适合处理海量数据和实时更新的场景。
不支持任意的偏移量
核心原理
1.排序依赖：必须指定至少一个排序字段（如时间戳、ID），结果按此排序。
2.状态传递：每次查询返回最后一条记录的排序值，客户端将其作为 search_after 参数传递给下一次请求。
3.无深度分页问题：无需维护全局页码，每次查询仅关注当前页之后的数据，避免 from/size 的性能开销。
注意：
1.排序字段必须包含唯一性：若排序字段值可能重复（如时间戳），需添加辅助字段（如 _id）确保唯一性，否则可能导致数据丢失。
2.不支持随机访问：search_after 只能按顺序向后翻页，无法直接跳转到指定页（需从头开始遍历）。
3.性能优化：优先使用 keyword 或数字类型字段排序（避免 text 字段的分词开销）。避免在高基数字段（如用户 ID）上进行深度分页。
4.实时性保证：每次查询反映最新数据状态（与 scroll 的快照机制不同）。

示例
1. 首次查询（获取第一页）
GET /products/_search
{
  "size": 10,  # 每页10条记录
  "sort": [
    {"timestamp": "desc"},  # 主排序字段（时间戳降序）
    {"_id": "asc"}          # 辅助排序字段（确保唯一性）
  ],
  "query": {
    "match": {
      "category": "electronics"
    }
  }
}
返回
{
  "hits": {
    "hits": [
      {
        "_id": "1001",
        "timestamp": 1680000000,
        "sort": [1680000000, "1001"]  // 最后一条记录的排序值
      },
      // ... 其他9条记录 ...
      {
        "_id": "1010",
        "timestamp": 1679999990,
        "sort": [1679999990, "1010"]  // 最后一条记录的排序值，用于下次查询
      }
    ]
  }
}
2. 使用 search_after 获取下一页
GET /products/_search
{
  "size": 10,
  "sort": [
    {"timestamp": "desc"},
    {"_id": "asc"}
  ],
  "search_after": [1679999990, "1010"],  // 上一页最后一条记录的排序值
  "query": {
    "match": {
      "category": "electronics"
    }
  }
}

ES提供了 Query 和 Filter 两种搜索：
Query Context：会对搜索进行相关性算分
Filter Context：不需要相关性算分，能够利用缓存来获得更好的性能

如果一个查询包含了多个不同字段的查询条件，可以使用bool 查询，一个 bool 查询可以包含一个或多个查询字句，支持以下四种查询：
must：必须匹配，贡献算分
should：选择性匹配，贡献算分
must_not：查询字句，必须不能匹配，不贡献算分
filter：必须匹配，不贡献算分

如果一个bool中只有should没有must，则should的n个条件中满足一个即可
如果bool中同时包含should和must，则should可以不满足，但满足的话会影响得分

term查询
不会分词，直接查询倒排索引中的词项。一般用于精准匹配
查询的字段最好设置为keyword（如果是text，查询结果可能不符合预期）
需确保查询词项与索引中的分词结果完全一致，否则无法命中
match查询
先对输入文本进行分词，再查每个词项（也是通过倒排索引），再汇总，默认按照score排序（query时）。一般用于全文搜索
比如文档包含“中文”，“中文2”，“中2”。使用默认分词器来查询“中文”可以将3条全部查询出来
综上，term最好在keyword字段上查询，而match最好在text字段上查询

wildcard
wildcard（通配符查询）是一种基于模式匹配的查询方式，允许使用 * 和 ? 通配符进行模糊搜索。‌不进行分词‌
*：匹配0个或多个任意字符‌（如 te*t 可匹配 test、text、teaaat）。
?：匹配1个任意字符‌（如 te?t 可匹配 test、text，但不能匹配 tet）。
最佳实践‌
避免 * 开头或过度复杂的通配模式。
对高频查询字段考虑使用 keyword 多字段（如 field.keyword）。
长文本模糊搜索建议改用 ngram 或 edge_ngram 分词器。
示例：GET /products/_search
{
  "query": {
    "wildcard": {
      "product_code": {
        "value": "pro*1"  // 匹配如 "pro1231"、"product1"
      }
    }
  }
}
通配符开头的模式（如 *test）会导致全索引扫描，性能极差。
默认区分大小写，若需忽略大小写，需结合 lowercase 分析器预处理数据。
替代方案‌：
前缀匹配：使用 prefix 查询（如 prefix: { "field": "pro" }）。
正则表达式：regexp 查询（功能更强但性能更低）。

默认分词器
会把中文分成单个汉字，因此查询类似“中国”会查询不到，但查询“中”能查到
会把大写转成小写，去掉“the”之类的单词
假设有文档{"content": "Hello World"}，则查询条件为"term": { "content": "Hello" }时无法查询到，必须使用hello才能

IK分词器
精准分词（ik_smart）：按最可能的方式切分文本。
示例："我爱北京天安门" → ["我", "爱", "北京", "天安门"]

细粒度分词（ik_max_word）：尽可能多的切分词语。
示例："我爱北京天安门" → ["我", "爱", "北京", "天安门", "天安", "门"]

es评分
评分计算是指在执行查询时，如何评估文档与查询的相关性，并为每个匹配的文档分配一个分数（_score）。这个分数越高，表示文档与查询的相关性越强。ES 默认使用 BM25 算法（从 Elasticsearch 5.0 版本开始）来计算评分
评分的基本原理
词频（Term Frequency, TF）：词项在文档中出现的次数。出现次数越多，相关性越高。
逆文档频率（Inverse Document Frequency, IDF）：词项在整个索引中的稀有性。越稀有（即越少文档包含该词），相关性越高。
字段长度归一化（Field-length Normalization）：字段越短，词项的重要性越高。例如，一个词在短标题中出现比在长正文中出现更有价值。
查询条件的组合：复合查询（如bool查询）会综合多个子查询的评分，使用不同的组合方式（如must、should、filter）。

可以使用explain参数来查看评分的详细计算过程：
GET /my_index/_search?explain
{
  "query": {
    "match": {
      "title": "Elasticsearch"
    }
  }
}

es节点
针对某个特定分片，es的节点可分为协调节点，主节点，复制节点，不同分片的主节点不同，类似rabbitmq的queue。
默认情况下，es的各个节点都可作为协调节点。客户端发送请求，则接收请求的节点自动成为协调节点，把请求分解成多个子操作，在各个节点执行（主节点先写入，然后同步给所有复制节点），等所有主节点和复制节点都执行成功，才返回响应给客户端
手动指定专用协调节点：节点的 elasticsearch.yml 新增或修改为 node.roles: [ ] 。这是官方推荐、符合集群设计原则、并能提供最佳性能和可靠性的方式。避免让客户端直接连接到承担存储职责的数据节点。
手动指定协调节点的优势：职责分离‌、‌负载均衡（可以在多个协调节点前放置负载均衡器，提高吞吐，避免单点故障）、简化客户端配置（客户端只需知道负载均衡器的地址）

es事务
Elasticsearch 本身不提供 严格的、关系型数据库意义上的 ACID 事务支持
es能保证对单个文档的操作（创建，更新，删除）是原子性的。但不能保证跨多个文档的操作的一致性

为什么不支持传统事务？‌
‌性能代价：‌ 实现分布式 ACID 事务（如两阶段提交 2PC）会带来巨大的性能开销（延迟增加、吞吐量下降）和复杂性，这与 ES 追求高性能、低延迟的核心目标相悖。
‌可用性权衡：‌ 强一致性事务往往会降低系统的可用性（在分区发生时需要牺牲可用性或一致性，CAP 定理）。ES 优先选择了分区容忍性和可用性 (AP)。
‌适用场景：‌ ES 的核心定位是搜索和分析海量数据，这些场景通常对近实时性和查询吞吐量要求极高，而对跨文档的强一致性事务需求相对较低。

_bulk API
使用 _bulk API 可以一次发送多个索引、更新、删除操作。
批量操作的部分原子性：‌ 使用 _bulk API 可以一次发送多个索引、更新、删除操作。_bulk API 本身不是原子事务：
‌单个操作独立：‌ 批量请求中的每个操作是独立执行的。一个操作失败（如版本冲突）不会自动回滚同一个批量请求中之前的操作。
‌整体非原子：‌ 如果批量请求在执行过程中部分成功、部分失败（例如某个操作冲突或节点故障），不会有回滚机制。成功执行的操作会被持久化，失败的操作需要应用层处理。
‌请求级原子性：‌ 从客户端角度看，_bulk 请求要么整体被 ES 接受（所有操作进入队列），要么整体被拒绝（如 JSON 格式错误）。但一旦请求被接受，其内部操作的执行是非原子的。
