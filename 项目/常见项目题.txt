
如何基于zookeeper实现配置中心和注册中心的
我们有一个专门的配置中心，叫做mc，manager center，启动时会读取专门的mc数据库，这里保存了全部的配置文件，然后一对一同步到zookeeper的znode节点，在mc的管理台可以更新配置文件，点击保存会保存到数据库，然后点击发布，会同步到znode节点
业务微服务的命令行参数里配置了mc的地址，启动时从zookeeper拉取配置，并监听这些znode节点，这样发布时就能感知到配置变化。这是配置中心的基本原理，接下来是注册中心，也是基于zk
业务微服务的命令行参数里还配置了appids参数，表示它负责哪些应用，比如abc，服务启动时在zk的abc节点下新建临时节点，内容为当前服务的ip端口等，这样实现了服务注册
我们还有一个专门的web容器，作为系统入口和路由，它启动时从zk的abc节点读取临时并监听节点，这样就知道哪些服务是可用的，这样实现了服务发现。接收浏览器的请求时，根据请求的appid找到套接字列表，根据负载均衡算法转发至其中一台。以上就是配置和注册中心的原理

DDD的领域有哪些，对应什么功能
基础服务云，如用户、组织、角色、权限
系统服务云，比如分布式锁、调度、敏感字段加密存储方案、敏感数据脱敏展示方案
集成服务云，即把一条数据从当前系统同步到另一个系统，需要定义集成对象，值转换规则，启动方案等
流程服务云，工作流相关，比如报销单审批，需要定义流程节点，流程编排等
还有其他业务相关的领域，比如发票云、财务云、HR云、供应链云、房地产云等
我们这个项目独有的云包括即时经营（和税相关）、博彩监察（赌场现场监管）、点算管理等

你遇到的最大的挑战，或者最大的亮点是什么
微众数据分析平台项目吧。难点在于每天都要写入共计10亿条数据的背景下，我们要保证数据不丢，也要保证不出现慢sql
保证数据不丢：我们有两个主要的系统a和b，a对外提供接口（接口支持单条和批量，也支持数据压缩），接收并校验数据并异步写入kafka；b会死循环地从kafka消费数据，写到LinkedBlockingQueue中，另外一组线程会消费这个队列，取出数据，按appId分组，校验数据写入tidb。也就是说基于kafka解耦，哪怕流量出现峰值，可以增加a系统的实例数量，防止接口超时，保证数据最终能够写入
保证不出现慢sql：单表数据很大，我们使用了tidb自带的分区，按照创建时间timestamp字段分区，一天一个区，数据最多保持30天，也就是30个分区，每个请求也要带上timestamp的查询范围，我们还限定死了查询的时间范围为0-24小时，防止跨分区。针对索引也做了很多轮优化，主要是把之前的多表join优化成了单表查询，索引中选择度高的字段放在前面，只保留必要的字段防止索引过大，多个索引合并减少索引的数量。由于大表禁止直接修改索引，我们还基于原表建了一个新的空表，空表修改了索引，然后自定义了一个迁移时间，是某一天的0时0分0秒，并对代码做了改造：写入时，如果当前时间小于迁移时间，则写入原表，如果当前时间大于迁移时间，则写入新表；查询时也是判断如果当前时间小于迁移时间，则查询原表，反之亦然。一开始我们用的mybatis，压测时发现dump文件中有大量的ibatis对象，所以后面放弃了mybatis，使用了原生的jdbc，省内存
